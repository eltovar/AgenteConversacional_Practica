# LLMClient - Documentaci√≥n T√©cnica

**Archivo**: `llm_client.py`
**Tipo**: Wrapper de LangChain para ChatOpenAI
**Estado**: üü° Funcional con deuda t√©cnica menor

---

## üìã √çndice

- [Descripci√≥n General](#descripci√≥n-general)
- [Arquitectura](#arquitectura)
- [Deuda T√©cnica Identificada](#deuda-t√©cnica-identificada)
- [Uso Actual en el Sistema](#uso-actual-en-el-sistema)
- [Comparaci√≥n T√©cnica](#comparaci√≥n-t√©cnica)
- [Recomendaciones](#recomendaciones)
- [Referencias](#referencias)

---

## Descripci√≥n General

`LLMClient` es un wrapper alrededor de `ChatOpenAI` de LangChain que proporciona una interfaz simplificada para interactuar con el modelo de lenguaje OpenAI (GPT-4o-mini).

### Funcionalidades

- Inicializaci√≥n centralizada del cliente OpenAI
- Validaci√≥n de `OPENAI_API_KEY`
- M√©todo `invoke()` para llamadas al LLM
- Soporte te√≥rico para tool calling (no utilizado en pr√°ctica)

### Ubicaci√≥n en el Proyecto

```
llm_client.py (32 l√≠neas)
‚îú‚îÄ‚îÄ Clase LLMClient
‚îÇ   ‚îú‚îÄ‚îÄ __init__()        # Inicializaci√≥n del cliente
‚îÇ   ‚îî‚îÄ‚îÄ invoke()          # Wrapper de invocaci√≥n
‚îÇ
‚îî‚îÄ‚îÄ llama_client          # Instancia global (Singleton)
```

---

## Arquitectura

### C√≥digo Actual

```python
# llm_client.py
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
from dotenv import load_dotenv
from typing import List
import os

load_dotenv()

class LLMClient:
    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("OPENAI_API_KEY no encontrada en .env")

        self.client = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            # API key se lee autom√°ticamente del entorno
        )

    # El wrapper de LangChain maneja la invocaci√≥n de tools nativamente
    def invoke(self, messages, tools=None, tool_choice=None):
        kwargs = {}
        if tools:
            kwargs["tools"] = tools
        if tool_choice:
            kwargs["tool_choice"] = tool_choice

        return self.client.invoke(messages, **kwargs)

# Instancia global
llama_client = LLMClient()
```

---

## Deuda T√©cnica Identificada

### üü° Duplicidad en Invocaci√≥n de Tools

**Severidad**: Menor - No afecta funcionalidad

**Descripci√≥n**:
El m√©todo `LLMClient.invoke()` incluye par√°metros `tools` y `tool_choice` que **NO se utilizan** en la arquitectura actual del sistema.

### An√°lisis del Problema

| Aspecto | Estado Actual | Impacto |
|---------|---------------|---------|
| **Funcionalidad** | ‚úÖ El sistema funciona correctamente | Ninguno |
| **Confusi√≥n** | ‚ö†Ô∏è Puede confundir a nuevos desarrolladores | Bajo |
| **Mantenibilidad** | ‚ö†Ô∏è C√≥digo no utilizado en el wrapper | Bajo |
| **Riesgo de bugs** | ‚úÖ No genera bugs | Ninguno |

---

## Uso Actual en el Sistema

### M√©todo 1: `bind_tools()` (LangChain 0.2+) - **Usado para Tool Calling**

#### Ubicaciones de Uso

**ReceptionAgent** - `reception_agent.py:69-72`:
```python
llm_with_tools = llama_client.client.bind_tools(
    [classify_intent_func],
    tool_choice="classify_intent"  # ‚Üê Forzado
)
response = llm_with_tools.invoke(messages)
```

**InfoAgent** - `info_agent.py:69-73`:
```python
llm_with_tools = llama_client.client.bind_tools(
    ALL_TOOLS,
    tool_choice="auto"  # ‚Üê Autom√°tico
)
response_llm = llm_with_tools.invoke(messages)
```

#### ¬øPor qu√© `bind_tools()` es el m√©todo correcto?

1. **Patr√≥n Chain de LangChain**: `bind_tools()` retorna un **nuevo objeto LLM modificado** con el esquema de tools preinyectado
2. **Tipo de retorno**: `Runnable[LanguageModelInput, BaseMessage]` - Permite composici√≥n de chains
3. **Soporte para tool_choice**: Permite especificar:
   - `"auto"` - LLM decide si usar tool
   - `"<tool_name>"` - Forzar uso de tool espec√≠fica
   - `"required"` - Forzar uso de alguna tool
4. **API OpenAI nativa**: Se traduce correctamente a `tools` y `tool_choice` en la llamada a OpenAI

---

### M√©todo 2: `llama_client.invoke()` - **Usado para Generaci√≥n Simple**

#### Ubicaci√≥n de Uso

**InfoAgent** - `info_agent.py:102`:
```python
final_response = llama_client.invoke(messages_rag).content
# ‚Üê NO pasa 'tools' ni 'tool_choice'
```

#### ¬øPor qu√© NO usa `bind_tools()` aqu√≠?

Porque **ya no necesita decidir sobre tools**. Esta es la fase de **generaci√≥n final** despu√©s de:
1. ‚úÖ Ya se decidi√≥ usar la tool RAG (`bind_tools()` en l√≠nea 69-73)
2. ‚úÖ Ya se ejecut√≥ el RAG y se obtuvo el contexto
3. ‚úÖ Ahora solo falta generar texto con ese contexto

---

## Comparaci√≥n T√©cnica

### Tabla Comparativa

| Aspecto | `bind_tools()` (Usado) | `invoke(tools=...)` (No usado) |
|---------|------------------------|-------------------------------|
| **Patr√≥n** | Chain composition | Direct invocation |
| **Retorno** | `Runnable` (chain) | `BaseMessage` |
| **Reutilizaci√≥n** | ‚úÖ S√≠ (crear `llm_with_tools` una vez) | ‚ùå No (pasar tools cada vez) |
| **tool_choice** | ‚úÖ Soporte completo | ‚ö†Ô∏è Funciona pero no idiom√°tico |
| **Versi√≥n LangChain** | 0.2+ (moderna) | 0.0.x-0.1.x (legacy) |
| **Documentaci√≥n oficial** | ‚úÖ Recomendado | ‚ö†Ô∏è Tolerado pero no promovido |
| **Uso en proyecto** | ‚úÖ ReceptionAgent, InfoAgent (decisi√≥n) | ‚úÖ InfoAgent (generaci√≥n final sin tools) |

---

## Flujo T√©cnico Completo

### InfoAgent - Dos Invocaciones Diferentes

| Fase | M√©todo Usado | Prop√≥sito | Requiere Tools | L√≠nea |
|------|--------------|-----------|----------------|-------|
| **1. Decisi√≥n** | `llama_client.client.bind_tools()` | ¬øUsar RAG o no? | ‚úÖ S√≠ | [69-73](../../info_agent.py#L69-L73) |
| **2. Generaci√≥n RAG** | `llama_client.invoke()` | Generar respuesta con contexto | ‚ùå No | [102](../../info_agent.py#L102) |
| **3. Generaci√≥n LLM Base** | (Usa respuesta de fase 1) | Responder directamente | ‚ùå No | [111](../../info_agent.py#L111) |

### Diagrama de Flujo

```mermaid
graph TD
    A[Usuario: ¬øCu√°l es la comisi√≥n?] --> B[process_info_query]

    B --> C[FASE 1: Decisi√≥n de Tool]
    C --> D[llama_client.client.bind_tools]
    D --> E[tool_choice='auto']
    E --> F[llm_with_tools.invoke]

    F --> G{response.tool_calls?}

    G -->|S√≠| H[FASE 2: Generaci√≥n RAG]
    H --> I[Ejecutar _run_tool]
    I --> J[llama_client.invoke - SIN tools]
    J --> K[Respuesta con contexto RAG]

    G -->|No| L[FASE 3: LLM Base]
    L --> M[Usar response_llm.content]
    M --> N[Respuesta conversacional]

    style D fill:#90EE90
    style J fill:#87CEEB
    style G fill:#FFD700
```

---

## Recomendaciones

### Opci√≥n 1: Documentar (Recomendado) ‚úÖ

**Acci√≥n**: Agregar docstring explicativo al m√©todo `invoke()`

**Implementaci√≥n**:

```python
def invoke(self, messages, tools=None, tool_choice=None):
    """
    Invoca el LLM para generaci√≥n de texto simple.

    IMPORTANTE: Este m√©todo NO se usa para tool calling avanzado.

    Para tool calling con decisi√≥n autom√°tica o forzada, usa bind_tools():

        # Decisi√≥n autom√°tica (InfoAgent)
        llm_with_tools = llama_client.client.bind_tools(tools, tool_choice="auto")
        response = llm_with_tools.invoke(messages)

        # Decisi√≥n forzada (ReceptionAgent)
        llm_with_tools = llama_client.client.bind_tools(tools, tool_choice="tool_name")
        response = llm_with_tools.invoke(messages)

    USO ACTUAL EN EL SISTEMA:
    - Generaci√≥n de texto final en InfoAgent (despu√©s de obtener contexto RAG)
    - Los par√°metros 'tools' y 'tool_choice' se mantienen por compatibilidad
      pero NO se utilizan en la arquitectura actual del sistema

    Args:
        messages: Lista de mensajes (SystemMessage, HumanMessage)
        tools: (Opcional) Lista de tools - NO USADO en arquitectura actual
        tool_choice: (Opcional) Estrategia de tool - NO USADO en arquitectura actual

    Returns:
        BaseMessage: Respuesta del LLM
    """
    kwargs = {}
    if tools:
        kwargs["tools"] = tools
    if tool_choice:
        kwargs["tool_choice"] = tool_choice

    return self.client.invoke(messages, **kwargs)
```

**Ventajas**:
- ‚úÖ No rompe compatibilidad
- ‚úÖ Documenta el patr√≥n correcto
- ‚úÖ Mantiene la simplificaci√≥n para casos sin tools
- ‚úÖ Aclara a futuros desarrolladores el uso correcto

**Desventajas**:
- ‚ö†Ô∏è Mantiene c√≥digo "muerto" para tool calling

---

### Opci√≥n 2: Eliminar Par√°metros de Tools (Refactor)

**Acci√≥n**: Simplificar m√©todo removiendo soporte de tools

**Implementaci√≥n**:

```python
def invoke(self, messages):
    """
    Invoca el LLM con mensajes (sin tools).

    Para tool calling, usa el patr√≥n bind_tools():
        llm_with_tools = llama_client.client.bind_tools(tools, tool_choice="auto")
        response = llm_with_tools.invoke(messages)

    Args:
        messages: Lista de mensajes (SystemMessage, HumanMessage)

    Returns:
        BaseMessage: Respuesta del LLM
    """
    return self.client.invoke(messages)
```

**Ventajas**:
- ‚úÖ Elimina c√≥digo no usado
- ‚úÖ Fuerza el patr√≥n correcto
- ‚úÖ C√≥digo m√°s limpio (5 l√≠neas vs 10)

**Desventajas**:
- ‚ö†Ô∏è Cambio potencialmente breaking si alguien usa `tools=...` (pero nadie lo hace actualmente)

---

### Opci√≥n 3: Hacer Nada (Status Quo)

**Acci√≥n**: Dejar el c√≥digo como est√°

**Ventajas**:
- ‚úÖ No requiere cambios
- ‚úÖ Funciona correctamente

**Desventajas**:
- ‚ùå Mantiene deuda t√©cnica
- ‚ùå Puede confundir a futuros desarrolladores

---

## Decisi√≥n Recomendada

**Opci√≥n 1: Documentar** con docstring

**Justificaci√≥n**:
1. ‚úÖ El sistema funciona perfectamente
2. ‚úÖ No hay riesgo de romper c√≥digo existente
3. ‚úÖ Mejora la claridad para futuros desarrolladores
4. ‚úÖ Mantiene el m√©todo `invoke()` simple para casos sin tools (su uso actual real)
5. ‚úÖ Documenta expl√≠citamente que `bind_tools()` es el patr√≥n correcto

---

## Referencias

### Documentaci√≥n Externa

- [LangChain bind_tools Documentation](https://python.langchain.com/docs/how_to/tool_calling/)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [LangChain ChatOpenAI API](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

### Documentaci√≥n Interna

- [PR 1: Refactor InfoAgent](../implementation/pr1_info_agent_refactor.md)
- [PR 2: Retry Logic](../implementation/pr2_retry_logic.md)
- [ReceptionAgent API](reception_agent.md)
- [InfoAgent API](info_agent.md)

---

## Historial de Cambios

| Fecha | Versi√≥n | Cambio | Autor |
|-------|---------|--------|-------|
| 2025-11-12 | 1.0.0 | Documentaci√≥n inicial de deuda t√©cnica | Claude Code |

---

**Autor**: Claude Code
**Fecha**: 2025-11-12
**Versi√≥n**: 1.0.0
**Estado**: üü° Pendiente de decisi√≥n sobre refactor
